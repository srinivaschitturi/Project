{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing variables and libraries. Importing data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "path=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_train_features.csv\"\n",
    "data=pd.read_csv(path)\n",
    "pathTest=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_test_features.csv\"\n",
    "dataTest=pd.read_csv(pathTest)\n",
    "XData=data.iloc[:,0:30].values\n",
    "XDataTest=dataTest.iloc[:,0:30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data and creating labels for overall survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    XData[:,i]=(XData[:,i]-np.mean(XData[:,i]))/(np.std(XData[:,i]))\n",
    "    XDataTest[:,i]=(XDataTest[:,i]-np.mean(XDataTest[:,i]))/(np.std(XDataTest[:,i]))\n",
    "bias=tf.Variable(tf.zeros(1,dtype=tf.float64))\n",
    "YData=np.transpose(np.matrix(np.float64(data.iloc[:,30].values)))\n",
    "YDataTest=np.transpose(np.matrix(np.float64(dataTest.iloc[:,30].values)))\n",
    "for i in range(98):\n",
    "    if YData[i,0]>=300:\n",
    "        YData[i,0]=1\n",
    "    else:\n",
    "        YData[i,0]=0\n",
    "\n",
    "for i in range(len(YDataTest)):   \n",
    "    if YDataTest[i,0]>=300:\n",
    "        YDataTest[i,0]=1\n",
    "    else:\n",
    "        YDataTest[i,0]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow placeholders and variables defined along with gradient descent optimization call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float64,shape=(None,30))\n",
    "Y=tf.placeholder(tf.float64,shape=(98,1))\n",
    "y_ = tf.placeholder(tf.float64, [33,1])\n",
    "weights=tf.Variable(tf.ones([30,1],tf.float64))\n",
    "yPred=tf.nn.sigmoid(-(tf.matmul(X,weights)+bias))\n",
    "loss=-tf.reduce_sum(Y*tf.log(yPred)+(1-Y)*tf.log(1-yPred))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow session with 1000 iterations. Accuracy is output using the same model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "[0.8947368421052632, 0.8571428571428571]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run([optimizer], feed_dict={X:XData, Y:YData})\n",
    "    yPred=tf.cast(tf.greater_equal(yPred,tf.constant(0.5,dtype=tf.float64)),tf.float64)\n",
    "    correct_prediction = tf.equal(yPred,y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    print(sess.run(accuracy, feed_dict={X: XDataTest, y_: YDataTest}))\n",
    "    prod=tf.multiply(yPred,y_)\n",
    "    summation=yPred+y_\n",
    "    ab=tf.cast(tf.size(y_),tf.float64)-tf.reduce_sum(y_)\n",
    "    specificity=tf.reduce_sum(tf.cast(tf.equal(yPred,tf.constant(0.0,tf.float64)),tf.float64))/ab\n",
    "    truepos=tf.reduce_sum(prod)\n",
    "    sensitivity=truepos/tf.reduce_sum(y_)\n",
    "    print(sess.run([sensitivity,specificity], feed_dict={X: XDataTest, y_: YDataTest}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81.81% accuracy is achieved on test data. Sensitivity value is 89.473% and specificity is 85.714%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar  2 21:54:25 2018\n",
    "\n",
    "@author: srini\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "path=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_train_features.csv\"\n",
    "data=pd.read_csv(path)\n",
    "pathTest=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_test_features.csv\"\n",
    "dataTest=pd.read_csv(pathTest)\n",
    "XData=data.iloc[:,0:30].values\n",
    "XDataTest=dataTest.iloc[:,0:30].values\n",
    "for i in range(30):\n",
    "    XData[:,i]=(XData[:,i]-np.mean(XData[:,i]))/(np.std(XData[:,i]))\n",
    "    XDataTest[:,i]=(XDataTest[:,i]-np.mean(XDataTest[:,i]))/(np.std(XDataTest[:,i]))\n",
    "bias=tf.Variable(tf.zeros(1,dtype=tf.float64))\n",
    "#XData=np.column_stack((XData,bias))\n",
    "YData=np.transpose(np.matrix(np.float64(data.iloc[:,30].values)))\n",
    "YDataTest=np.transpose(np.matrix(np.float64(dataTest.iloc[:,30].values)))\n",
    "for i in range(98):\n",
    "    if YData[i,0]>=300:\n",
    "        YData[i,0]=1\n",
    "    #elif (YData[i,0]>=300) & (YData[i,0]<450):\n",
    "    #    YData[i,0]=1\n",
    "    else:\n",
    "        YData[i,0]=0\n",
    "\n",
    "for i in range(len(YDataTest)):   \n",
    "    if YDataTest[i,0]>=300:\n",
    "        YDataTest[i,0]=1\n",
    "    #elif (YDataTest[i,0]>=300) & (YDataTest[i,0]<450:\n",
    "    #    YDataTest[i,0]=1\n",
    "    else:\n",
    "        YDataTest[i,0]=0\n",
    "X=tf.placeholder(tf.float64,shape=(None,30))\n",
    "Y=tf.placeholder(tf.float64,shape=(98,1))\n",
    "y_ = tf.placeholder(tf.float64, [33,1])\n",
    "weights=tf.Variable(tf.ones([30,1],tf.float64))\n",
    "yPred=tf.nn.sigmoid(-(tf.matmul(X,weights)+bias))\n",
    "loss=-tf.reduce_sum(Y*tf.log(yPred)+(1-Y)*tf.log(1-yPred))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run([optimizer], feed_dict={X:XData, Y:YData})\n",
    "    #weights, loss, bias=sess.run([weights,loss,bias], feed_dict={X:XData, Y:YData})\n",
    "    #yPred=1/(1+tf.exp(-tf.matmul(XData,weights)))\n",
    "    #yPred=tf.cast(tf.greater_equal(yPred,tf.constant(0.5,dtype=tf.float64)),tf.float64)\n",
    "    #pred, truth=sess.run([yPred,Y], feed_dict={X:XData, Y:YData})\n",
    "    #print(np.mean(pred==truth))\n",
    "#    yPredTest=1/(1+np.exp(-(np.dot(XDataTest,weights)+bias)))\n",
    "#    yPredTest=np.cast['f'](yPredTest>0.5)\n",
    "#    print(np.mean(yPredTest==YDataTest))\n",
    "    correct_prediction = tf.equal(tf.argmax(yPred,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    print(sess.run(accuracy, feed_dict={X: XDataTest, y_: YDataTest}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "[0.8947368421052632, 0.8571428571428571]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "path=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_train_features.csv\"\n",
    "data=pd.read_csv(path)\n",
    "pathTest=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_test_features.csv\"\n",
    "dataTest=pd.read_csv(pathTest)\n",
    "XData=data.iloc[:,0:30].values\n",
    "XDataTest=dataTest.iloc[:,0:30].values\n",
    "for i in range(30):\n",
    "    XData[:,i]=(XData[:,i]-np.mean(XData[:,i]))/(np.std(XData[:,i]))\n",
    "    XDataTest[:,i]=(XDataTest[:,i]-np.mean(XDataTest[:,i]))/(np.std(XDataTest[:,i]))\n",
    "bias=tf.Variable(tf.zeros(1,dtype=tf.float64))\n",
    "YData=np.transpose(np.matrix(np.float64(data.iloc[:,30].values)))\n",
    "YDataTest=np.transpose(np.matrix(np.float64(dataTest.iloc[:,30].values)))\n",
    "for i in range(98):\n",
    "    if YData[i,0]>=300:\n",
    "        YData[i,0]=1\n",
    "    else:\n",
    "        YData[i,0]=0\n",
    "\n",
    "for i in range(len(YDataTest)):   \n",
    "    if YDataTest[i,0]>=300:\n",
    "        YDataTest[i,0]=1\n",
    "    else:\n",
    "        YDataTest[i,0]=0\n",
    "X=tf.placeholder(tf.float64,shape=(None,30))\n",
    "Y=tf.placeholder(tf.float64,shape=(98,1))\n",
    "y_ = tf.placeholder(tf.float64, [33,1])\n",
    "weights=tf.Variable(tf.ones([30,1],tf.float64))\n",
    "yPred=tf.nn.sigmoid(-(tf.matmul(X,weights)+bias))\n",
    "loss=-tf.reduce_sum(Y*tf.log(yPred)+(1-Y)*tf.log(1-yPred))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0005).minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run([optimizer], feed_dict={X:XData, Y:YData})\n",
    "    yPred=tf.cast(tf.greater_equal(yPred,tf.constant(0.5,dtype=tf.float64)),tf.float64)\n",
    "    correct_prediction = tf.equal(yPred,y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    print(sess.run(accuracy, feed_dict={X: XDataTest, y_: YDataTest}))\n",
    "    prod=tf.multiply(yPred,y_)\n",
    "    summation=yPred+y_\n",
    "    ab=tf.cast(tf.size(y_),tf.float64)-tf.reduce_sum(y_)\n",
    "    specificity=tf.reduce_sum(tf.cast(tf.equal(yPred,tf.constant(0.0,tf.float64)),tf.float64))/ab\n",
    "    truepos=tf.reduce_sum(prod)\n",
    "    sensitivity=truepos/tf.reduce_sum(y_)\n",
    "    print(sess.run([sensitivity,specificity], feed_dict={X: XDataTest, y_: YDataTest}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5454545454545454\n",
      "0.6428571428571429\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar  2 21:54:25 2018\n",
    "\n",
    "@author: srini\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "path=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_train_features.csv\"\n",
    "data=pd.read_csv(path)\n",
    "pathTest=\"C:\\\\Users\\\\srini\\\\OneDrive\\\\Academics\\\\ML courses\\\\ID 5030\\\\Assignments\\\\Assignment 3\\\\30_test_features.csv\"\n",
    "dataTest=pd.read_csv(pathTest)\n",
    "XData=data.iloc[:,0:30].values\n",
    "XDataTest=dataTest.iloc[:,0:30].values\n",
    "for i in range(30):\n",
    "    XData[:,i]=(XData[:,i]-np.mean(XData[:,i]))/(np.std(XData[:,i]))\n",
    "    XDataTest[:,i]=(XDataTest[:,i]-np.mean(XDataTest[:,i]))/(np.std(XDataTest[:,i]))\n",
    "bias=tf.Variable(tf.zeros(3,dtype=tf.float64))\n",
    "extra=np.zeros([98,1])\n",
    "extraTest=np.zeros([33,1])\n",
    "YData=np.column_stack((np.transpose(np.matrix(np.float64(data.iloc[:,30].values))),extra,extra))\n",
    "YDataTest=np.column_stack((np.transpose(np.matrix(np.float64(dataTest.iloc[:,30].values))),extraTest,extraTest))\n",
    "for i in range(98):\n",
    "    if YData[i,0]>=450:\n",
    "        YData[i,0]=1\n",
    "        YData[i,1]=0\n",
    "        YData[i,2]=0\n",
    "    elif (YData[i,0]>=300) & (YData[i,0]<450):\n",
    "        YData[i,0]=0\n",
    "        YData[i,1]=1\n",
    "        YData[i,2]=0\n",
    "    else:\n",
    "        YData[i,0]=0\n",
    "        YData[i,1]=0\n",
    "        YData[i,2]=1\n",
    "\n",
    "for i in range(len(YDataTest)):   \n",
    "    if YDataTest[i,0]>=450:\n",
    "        YDataTest[i,0]=1\n",
    "        YDataTest[i,1]=0\n",
    "        YDataTest[i,2]=0\n",
    "    elif (YDataTest[i,0]>=300) & (YDataTest[i,0]<450):\n",
    "        YDataTest[i,0]=0\n",
    "        YDataTest[i,1]=1\n",
    "        YDataTest[i,2]=0\n",
    "    else:\n",
    "        YDataTest[i,0]=0\n",
    "        YDataTest[i,1]=0\n",
    "        YDataTest[i,2]=1\n",
    "\n",
    "X=tf.placeholder(tf.float64,shape=(None,30))\n",
    "Y=tf.placeholder(tf.float64,shape=(None,3))\n",
    "weights=tf.Variable(tf.ones([30,3],tf.float64))\n",
    "yPred=tf.nn.softmax(tf.matmul(X,weights)+bias)  \n",
    "loss=-tf.reduce_sum(Y*tf.log(yPred)+(1-Y)*tf.log(1-yPred))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run([optimizer], feed_dict={X:XData, Y:YData})\n",
    "    correct_prediction = tf.equal(tf.argmax(yPred,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    print(sess.run(accuracy, feed_dict={X: XDataTest, Y: YDataTest}))\n",
    "    yPredn=tf.one_hot(tf.argmax(yPred,1),3)\n",
    "    predn, act= sess.run([yPredn,Y], feed_dict={X: XDataTest, Y: YDataTest})\n",
    "    sens=0\n",
    "    count=0\n",
    "    for i in range(33):\n",
    "        if (i==14) | (i==24) | (i==32):\n",
    "            print(sens/count)\n",
    "            sens=0\n",
    "            count=0\n",
    "        if(np.all(predn[i]==act[i])):\n",
    "            sens+=1\n",
    "        count+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multinomial logistic regression only an accuracy of 54.54% could be achieved. Sensitivity value with respect to each outcome (<300, 300<x<450, >450) are obtained as 0.6428, 0.5, 0.5 respectively. \n",
    "Extremely low accuracy can be attributed to the very high number of features to be trained and very less training data with multiple target classes as well. Performance of model is poor with the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
